{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dill as pickle\n",
    "\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573226b",
   "metadata": {},
   "source": [
    "<h3>Layer Class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.name = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def backward(self, gradient, lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b623f393",
   "metadata": {},
   "source": [
    "<h3>Linear Layer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8295458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    \n",
    "    def __init__(self, x_size, y_size):\n",
    "        self.name = 'Linear_Layer'\n",
    "        self.weights = np.random.rand(y_size, x_size)\n",
    "        #Zeors\n",
    "        #self.weights = np.zeros((y_size, x_size))\n",
    "        #Weights\n",
    "        #self.weights = np.random.uniform(low=-10, high=10, size=(y_size,x_size))\n",
    "        self.bias = np.random.rand(y_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.dot(self.weights, self.x) + self.bias\n",
    "    \n",
    "    def backward(self, gradient, lr=0.01):\n",
    "        weights_grad = np.dot(gradient, self.x.T)\n",
    "        x_grad = np.dot(self.weights.T, gradient)\n",
    "        self.weights -= lr * weights_grad\n",
    "        self.bias -= lr * gradient\n",
    "        return x_grad\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f68b96",
   "metadata": {},
   "source": [
    "<h3> Sigmoid Activation Layer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivation(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Sigmoid_Activation'\n",
    "        self.sigmoid = lambda x : 1. / (1. + np.exp(-x))\n",
    "        self.sigmoid_grad = lambda x : self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.sigmoid(self.x)\n",
    "    \n",
    "    def backward(self, gradient, lr=0.01):\n",
    "        return np.multiply(gradient,self.sigmoid_grad(self.x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa45499",
   "metadata": {},
   "source": [
    "<h3>Hyperbolic Tangent Activation Layer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhActivation(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Tanh_Activation'\n",
    "        self.tanh = lambda x : np.tanh(x)\n",
    "        self.tanh_grad = lambda x : 1. - np.tanh(x)**2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.tanh(self.x)\n",
    "    \n",
    "    def backward(self, gradient, lr=0.01):\n",
    "        return np.multiply(gradient,self.tanh_grad(self.x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c1eb0",
   "metadata": {},
   "source": [
    "<h3>Cross Entropy Loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.name = 'CrossEntropyLoss'\n",
    "\n",
    "    def cross_entropy(self, y_actual, y_pred):\n",
    "        return np.mean(-y_actual * np.log(y_pred))\n",
    "    \n",
    "    def cross_entropy_grad(self, y_actual, y_pred):\n",
    "        return y_actual - y_pred\n",
    "    \n",
    "    def forward(self, y_actual, y_pred):\n",
    "        self.y = y_pred\n",
    "        return self.cross_entropy(y_actual, y_pred)\n",
    "    \n",
    "    def backward(self, y_actual, y_pred):\n",
    "        return self.cross_entropy_grad(y_actual, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada071c",
   "metadata": {},
   "source": [
    "<h3>Soft Max Layer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax:\n",
    "    def __init__(self):\n",
    "        self.name = 'SoftMax'\n",
    "\n",
    "    def forward(self, y_pred):\n",
    "        cls_prob = np.exp(y_pred)\n",
    "        tot_prob = np.sum(cls_prob)\n",
    "        return cls_prob/tot_prob\n",
    "    \n",
    "    def backward(self, y_pred):\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e5e0c",
   "metadata": {},
   "source": [
    "<h3>Sequential Layer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Layer):\n",
    "    def __init__(self, network=[]):\n",
    "        self.name = 'Sequential'\n",
    "        self.network = network\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.network:\n",
    "            #print(f'''layer-{layer.name} shape:{out.shape}''')\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, gradient, lr = 0.01):\n",
    "        grad = gradient\n",
    "        for layer in reversed(self.network):\n",
    "            grad = layer.backward(grad, lr)\n",
    "        return grad\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19b347",
   "metadata": {},
   "source": [
    "<h3>Utility Methods </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EARLY_STOP_LIMIT = 5\n",
    "NUMBER_OF_CLASSES = 10\n",
    "\n",
    "def early_stopping_required(loss_list = []):\n",
    "    if len(loss_list) >= EARLY_STOP_LIMIT + 1:\n",
    "        recent_loss_val = loss_list[-1]\n",
    "        count = 0\n",
    "        for loss in loss_list[-5:]:\n",
    "            if recent_loss_val >= loss:\n",
    "                count+=1\n",
    "        if count == 5:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def train(sequential_network, x_train, y_train, epochs = 2000, lr =0.01):\n",
    "    loss_list =[]\n",
    "    for itr in range(epochs):\n",
    "        error = 0\n",
    "        for x, y_actual in zip(x_train, y_train):\n",
    "            y_pred = sequential_network.forward(x)\n",
    "            #print('y---pred',y_pred)\n",
    "            loss = CrossEntropyLoss()\n",
    "            error+= loss.cross_entropy(y_actual, y_pred)\n",
    "            #print('error: ',error)\n",
    "            gradient_loss = loss.cross_entropy_grad(y_actual, y_pred)\n",
    "            sequential_network.backward(gradient_loss,lr=0.01)\n",
    "        error /= len(x_train)\n",
    "        loss_list.append(error)\n",
    "        #Early Stopping\n",
    "        if early_stopping_required(loss_list):\n",
    "            epochs = itr + 1\n",
    "            print(f'''Early Stopping the Model Training at Epoch:{epochs}''')\n",
    "            break\n",
    "        #print(f\"{itr + 1}/{epochs}, error={error}\")\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(211)\n",
    "    ax.plot(np.arange(0,epochs),np.array(loss_list).reshape(-1))\n",
    "    ax.set_title('Training Loss')\n",
    "    return sequential_network\n",
    "\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "def preprocess(samples, targets):\n",
    "    # reshape and normalization\n",
    "    samples = samples.reshape(samples.shape[0], 28 * 28, 1)\n",
    "    samples = samples.astype(\"float32\") / 255\n",
    "    #convery to numbers\n",
    "    conv_targets = [int(num) for num in targets]\n",
    "    conv_targets = np.array(conv_targets)\n",
    "    targets_vect = get_one_hot(conv_targets,NUMBER_OF_CLASSES)\n",
    "    targets_vect = targets_vect.reshape(targets_vect.shape[0], 10, 1)\n",
    "    return samples, targets_vect\\\n",
    "\n",
    "def predict_xor(model, x_test,y_actual):\n",
    "    score = 0\n",
    "    for idx, test_sample in enumerate(x_test):\n",
    "        y_pred = model.forward(test_sample)\n",
    "        #print(f'''Test Sample - {test_sample} \\n Model Predicted Value- {y_pred}''')\n",
    "        softmax = SoftMax()\n",
    "        probs = softmax.forward(y_pred)\n",
    "        print(f'''Actual value - {y_actual[idx]} Predicted Value- {np.argmax(probs)}''')\n",
    "        if np.argmax(probs) == y_actual[idx]:\n",
    "            score+=1\n",
    "    return score/len(y_actual)\n",
    "\n",
    "def predict(model, x_test,y_actual):\n",
    "    score = 0\n",
    "    for idx, test_sample in enumerate(x_test):\n",
    "        y_pred = model.forward(test_sample)\n",
    "        #print(f'''Test Sample - {test_sample} \\n Model Predicted Value- {y_pred}''')\n",
    "        softmax = SoftMax()\n",
    "        probs = softmax.forward(y_pred)\n",
    "        #print(f'''Actual value - {np.argmax(y_actual[idx])} Predicted Value- {np.argmax(probs)}''')\n",
    "        if np.argmax(probs) == np.argmax(y_actual[idx]):\n",
    "            score+=1\n",
    "    return score/len(y_actual)\n",
    "\n",
    "def plot_validation_loss(model, x_val,y_val):\n",
    "    epochs = 200\n",
    "    error_mean = []\n",
    "    for itr in range(0,epochs):\n",
    "        error_list = []\n",
    "        for idx, val_sample in enumerate(x_val):\n",
    "            val_pred = model.forward(val_sample)\n",
    "            loss = CrossEntropyLoss()\n",
    "            error = loss.cross_entropy(val_pred, y_val[idx])\n",
    "            error_list.append(error)\n",
    "        error_mean.append(np.mean(error_list))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(221)\n",
    "    ax.plot(np.arange(0,epochs),np.array(error_mean).reshape(-1))\n",
    "    ax.set_title('Validation Loss')\n",
    "    \n",
    "\n",
    "def save(model, filename):\n",
    "    pickle.dump(model,open(filename, 'wb'))\n",
    "\n",
    "def load(filename):\n",
    "    return pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0cc024",
   "metadata": {},
   "source": [
    "# 1. XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01303f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XOR inputs \n",
    "samples = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0]])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(samples[:, 0], samples[:, 1], c=targets)\n",
    "ax.set_title('Xor data distribution')\n",
    "\n",
    "model = Sequential([\n",
    "    LinearLayer(2,2),\n",
    "    SigmoidActivation(),\n",
    "    LinearLayer(2,1),\n",
    "    SigmoidActivation() ])\n",
    "\n",
    "\n",
    "model_tanh = Sequential([\n",
    "    LinearLayer(2,2),\n",
    "    TanhActivation(),\n",
    "    LinearLayer(2,1),\n",
    "    TanhActivation() ])\n",
    "\n",
    "trained_model = train(model,samples.reshape(4,2,1),targets.reshape(4,1,1),epochs=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8ea08",
   "metadata": {},
   "source": [
    "<h3>Save and Load the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd867484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'XOR_solved.w'\n",
    "save(trained_model,filename)\n",
    "mp = load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ae302",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = predict_xor(trained_model,samples.reshape(4,2,1),targets.reshape(4,1,1))\n",
    "print(' Model Score: ',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb00b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xor(mp,samples.reshape(4,2,1),targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.reshape(4,1,1)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df41c56",
   "metadata": {},
   "source": [
    "# 2. Hand Written Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe907b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b571e",
   "metadata": {},
   "source": [
    "<h3>Training , Validation and Test datasets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add6bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_samples, mnist_targets = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "X, Y = preprocess(mnist_samples,mnist_targets)\n",
    "#Training Split \n",
    "train_split,combine_split,train_y_split,combine_y_split = train_test_split(X, Y,test_size=0.2, random_state=123)\n",
    "#Validation Split\n",
    "val_split,test_split,val_y_split,test_y_split = train_test_split(combine_split, combine_y_split,test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35934a1c",
   "metadata": {},
   "source": [
    "<h3>Shapes of Training, Validation and Test datasets </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77261a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Sample\n",
    "print('*'*25)\n",
    "print('Train sample split shape', train_split.shape)\n",
    "print('Train targets split shape' ,train_y_split.shape)\n",
    "print('*'*25)\n",
    "# Validation Samples\n",
    "print('Validation sample split shape' ,val_split.shape)\n",
    "print('Validation targets split shape', val_split.shape)\n",
    "print('*'*25)\n",
    "# Test Samples\n",
    "print('Test sample split shape' ,test_split.shape)\n",
    "print('Test targets split shape', test_split.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2533281",
   "metadata": {},
   "source": [
    "<h3>Training Models with differerent layers and HyperParameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfa861",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model_1 = Sequential([\n",
    "    LinearLayer(28*28,40),\n",
    "    SigmoidActivation(),\n",
    "    LinearLayer(40,10),\n",
    "    SigmoidActivation()])\n",
    "\n",
    "mnist_model_2 = Sequential([\n",
    "    LinearLayer(28*28,40),\n",
    "    TanhActivation(),\n",
    "    LinearLayer(40,10),\n",
    "    TanhActivation()])\n",
    "\n",
    "mnist_model_3 = Sequential([\n",
    "    LinearLayer(28*28,40),\n",
    "    TanhActivation(),\n",
    "    LinearLayer(40,20),\n",
    "    TanhActivation(),\n",
    "    LinearLayer(20,10),\n",
    "    TanhActivation()])\n",
    "\n",
    "trained_model_1 = train(mnist_model_1,train_split,train_y_split,epochs=10, lr=0.001)\n",
    "\n",
    "trained_model_2 = train(mnist_model_2,train_split,train_y_split,epochs=10, lr=0.001)\n",
    "\n",
    "trained_model_3 = train(mnist_model_3,train_split,train_y_split,epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554351a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_validation_loss(trained_model_1, val_split,val_y_split)\n",
    "#plot_validation_loss(trained_model_2, val_split,val_y_split)\n",
    "#plot_validation_loss(trained_model_3, val_split,val_y_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model_1 = predict(trained_model_1,test_split,test_y_split)\n",
    "print('Model_1 Score: ',round(score_model_1,4))\n",
    "\n",
    "score_model_2 = predict(trained_model_2,test_split,test_y_split)\n",
    "print('Model_2 Score: ',round(score_model_2,4))\n",
    "\n",
    "score_model_3 = predict(trained_model_3,test_split,test_y_split)\n",
    "print('Model_3 Score: ',round(score_model_3,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_file = 'mnist_solved.w'\n",
    "save(trained_model_2,mnist_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538eae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_saved_model =  load(mnist_file )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cse6363] *",
   "language": "python",
   "name": "conda-env-cse6363-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
